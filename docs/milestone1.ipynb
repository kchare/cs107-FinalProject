{"cells":[{"cell_type":"markdown","source":"## Introduction\nThis software package `Boomeraang_optimizer` implements an optimization of a user-supplied or pre-set objective function. Many scientific and social scientific fields rely on probabilistic and statistical methodologies that fundamentally concern the optimization of some function, often called an objective function or a cost function. Intuitively, these methods seek to find the best fit of some function given observed data. Two common point estimators in data-driven fields are Maximum Likelihood Estimator (MLE) from Frequentist's view and Maximum A Posterior (MAP) from Bayesian's view. In each case, the optimum points are identified by the stationary condition (first order derivative equals 0) and convexity check by higher order derivatives. Generally, a major branch of modern optimization methods, like Stochastic Gradient Descent (SGD) and Broyden–Fletcher–Goldfarb–Shanno algorithm(BFGS), are established via the first or higher order gradient of target function, thus heavily relies on an efficient gradient computation.\n\nAuto Differentiation (AD) is one highly effective method for calculation of the gradient function at some point. The method, which balances the efficiency of numeric computation and the precision of symbolic derivatives, is commonly used for optimization applications. Our library solves the optimization problem described above via gradient descent, which is implemented on top of a forward-mode autodifferentiation. The advance of this method is that AD can effectively and efficiently compute the Jacobian matrix. For the optimization problems we consider, the multidimensional nature of the challenge necessarily lends itself to the use of AD.\n","metadata":{"tags":[],"cell_id":"00001-77ff2426-48f1-4e45-a8f2-639ad171aa58"}},{"cell_type":"markdown","source":"## Background\nAt it’s heart, automatic differentiation (AD) seeks to calculate the derivative of some function, and evaluate both the function and the derivative, at a given point by iteratively applying the chain rule to a composition of elementary functions whose derivatives are well-known. This application of the chain rule [https://en.wikipedia.org/wiki/Chain_rule] is essential. By viewing a more complex function as simply the composition of many elementary functions, the calculation of the derivative becomes a series of steps, starting from the innermost function in the forward mode of AD. In each step, the chain rule is applied. For $z(y(x))$:\n\n$$ \\frac{dz}{dx} = \\frac{dz}{dy} \\frac{dy}{dx} $$\n\nImportantly, because $z(y)$ will be an elementary function (e.g. addition, subtraction, sine, cosine), its derivative can be easily calculated. As we have begun at the innermost derivative, that function $\\frac{dy}{dx}$ is known and can be used to iteratively calculate the derivative of the composition. For the simple example above, there are only two elementary operations, but this method can be extended to cover many elementary operations. One only needs to keep track of the derivative of each ‘running’ piece.\n\nThe method can be implemented through a graph structure (see below for a simple example), with each node in the graph, which represents a single elementary operation. This computational graph is especially important when the function of interest relies on multiple elements (e.g. $f(x,y) = x^2 + \\sin(y)$). One particular advantage of the elementary operations in this method is that each type of operation has a known derivative that can be calculated systematically and efficiently. At each step in the forward mode, the algorithm only needs to maintain the status of the derivative (potentially multiple partial derivatives in the case of multiple elements) as well as the current value of the function.\n\nThese storage requirements and iterative nature take advantage of a computer’s ability to store many values and perform many simple operations very quickly. One challenge for computers in calculating complex, symbolic derivatives is that symbolic differentiation may lead to enormous equations and syntactical rules that are highly complex to apply. For humans with a pen and paper, the AD approach may take too much time. The number of calculations, albeit simple, would certainly overwhelm the ability of most people to simply calculate the symbolic derivatives and implement a single evaluation. That concern is severely attenuated by computers. Additionally, AD is superior to the finite differences method of differentiation due to the fact that it is able to keep track of derivatives and functions at the level of machine precision. In scientific applications, this feature is incredibly important, as the sensitive systems measured or engineered will not be successful with only generalities.\n\n","metadata":{"tags":[],"cell_id":"00001-9bb640df-ede6-4110-a343-a508005abe68"}},{"cell_type":"markdown","source":"## How to use *boom-diff* and *Boomeraang-optimizer*\n\n*What should they import? *\nThey should import the entire autodiff module.\n\n\n*How can they instantiate AD objects? *\nUsers will be able to instantiate objects in two ways:\n\n1. A user will be able to implement a function and a set of points where it should be implemented. The autodiff module will then differentiate the function at the point supplied by the user.\n\n2. Users can also instantiate classes directly as an AD class object. This will allow the user to iteratively build the autodifferentiation.\n\nLinearRegression will take in its fit method arrays X, y and will store the coefficients  of the linear model in its coef_ member:\n```\n >>> from boom-diff import autodiff\n >>> func = \"x^2\" #lambda x: x**2 \n >>> ad = autodiff.AD(func, {x:2})\n >>> ad.diff\n     (4, 4)\n```","metadata":{"tags":[],"cell_id":"00002-81ef99a6-cb0a-420a-a0be-47e96df022d3"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00004-adac51aa-9711-4faa-bf99-03cb46754385"}},{"cell_type":"markdown","source":"## Software organization\n\nWhat will the directory structure look like?\nThe implementation for our software will follow the structure laid out below:\n```\ncs107-FinalProject/\n\tboomeraang-optimizer/\n\t\t__init__.py\n\t\t[implement optimization].py\n\t\tsetup.py\n\tboom-diff/\n\t\t__init__.py\n\t\tautodiff.py\n\t\tsetup.py\n\t\t\t\t\t\n\tdocs/\n\t    milestone1.md\n\t    documentation.md\n\ttests/\n\t\tunit_tests.py\n```\n\n \n- *What modules do you plan on including? What is their basic functionality?*\n The code will have two modules: first will implement automatic differentiation (boom-diff) and the second will perform optimization using the AD.\n \n- *Where will your test suite live? Will you use TravisCI? CodeCov?*\n We are planning to use TravisCI and CodeCov to make sure that our code can be used by others. The test suites will be on GitHub, to ensure compatibility of different versions.\n\n- *How will you distribute your package (e.g. PyPI)?*\n We should only need a simple package that could be installed using pip. In order to create one, we are planning to follow the tutorial from this website: https://python-packaging.readthedocs.io/en/latest/index.html.\n  \n- *How will you package your software? Will you use a framework? If so, which one and why? If not, why not?*\n We’re not planning to use the framework since our package is not going to be a web application. Also, the package should be basic enough and contain all required documentation. \n","metadata":{"tags":[],"cell_id":"00005-8ef402ec-afd3-45bf-999e-6678c112d396"}},{"cell_type":"markdown","source":"## Implementation\n\n- Core data structures:\n    - Class for objective function\n    - Create function/AD class. With operation overloading, this will be used to 'recreate' the objective function from the inside out\n    - Potentially class for specific methods (could also use a class method to construct objective function class)\n    - Do we want to consider differences for scalars and vectors?\n- Classes\n    - ObjectiveFunction\n        - Methods\n            - set_function(): allows user to set objective function\n                - Validation that function\n            - optimize(method={'graddesc', etc): run optimization; return value of optimization\n            - from_X(): where 'X' represents some preset function (may have multiple)\n        - Attributes:\n            - data\n            - functional_form\n    - DiffFunc()\n        - Attributes:\n            - point: location for differentiation\n            - deriv: derivative at point\n            - (do we need this?) trace_table: method for storing trace table...list of list?\n        - Methods:\n            - Overloading for common methods\n            - Additional functions for implementation of elementary operations (do we want to restrict?)\n- External dependencies\n    - numpy\n    - scipy\n    - matplotlib\n- Elementary functions:\n    - Will rely on numpy execution for calling results of functions.\n    - Define set of Python functions to handle basic derivatives\n ","metadata":{"tags":[],"cell_id":"00006-577c2d54-2d10-4174-9dd2-66c084064801"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00007-b142309f-96f4-43d3-9926-4017d3c2ee0f"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"c529a829-04cb-4fbc-8e3e-3aa8731333ba","deepnote_execution_queue":[]}}